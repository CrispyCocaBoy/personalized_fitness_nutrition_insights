services:

  # Frontend
  streamlit_app:
    build:
      context: .
      dockerfile: ./src/frontend/Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./src/frontend:/app
      - ./utility:/app/utility
    depends_on:
      - cockroachdb


## Kafka
  broker_kafka:
    image: apache/kafka:latest
    container_name: broker_kafka
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://broker_kafka:9092,CONTROLLER://broker_kafka:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker_kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker_kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - ./volumes/kafka_setting/broker:/var/lib/kafka

  init_kafka:
    build:
      context: .
      dockerfile: ./src/kafka_topics/Dockerfile
    depends_on:
      - broker_kafka
    container_name: init_kafka

# Databases

  # SQL-type
  cockroachdb:
    image: cockroachdb/cockroach:v25.2.0
    container_name: cockroachdb
    command: start-single-node --insecure --http-addr=0.0.0.0:8080
    ports:
      - "26257:26257" # Porta SQL
      - "8087:8080"   # Admin UI
    volumes:
      - ./volumes/cockroach_data:/cockroach/cockroach-data

  # Datalakehouse
  minio:
    image: minio/minio
    ports:
      - "9000:9000"      # API
      - "9001:9001"      # Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./volumes/minio_setting:/data

  # Cache
  ## Redis community
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"         # Porta standard Redis
    volumes:
      - ./volumes/redis_data:/data
    restart: unless-stopped



  # Change Data Capture
  ## Kafka connector
  change_data_capture_user_map:
    build:
      context: ./src/change_data_capture/CDC_user_map
      dockerfile: Dockerfile
    container_name: change_data_capture_user_map
    depends_on:
      - broker_kafka
      - redis
    environment:
      KAFKA_BOOTSTRAP: broker_kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: 6379

  change_data_capture_toggling_sensor:
    build:
      context: ./src/change_data_capture/CDC_toggling_sensor
      dockerfile: Dockerfile
    container_name: change_data_capture_toggling_sensor
    depends_on:
      - broker_kafka
      - redis
    environment:
      KAFKA_BOOTSTRAP: broker_kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: 6379



# ETL process
  ## Spark Master
  spark-master:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-master
    command: /opt/bitnami/scripts/spark/run.sh master
    environment:
      - SPARK_MODE=master
    ports:
      - "8085:8080"
      - "7077:7077"

  ## Spark Worker 1
  spark-worker-1:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-worker-1
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=3
    depends_on:
      - spark-master

  ## Spark Worker 2
  spark-worker-2:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-worker-2
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master

  spark-worker-3:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-worker-3
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master


# Gateway
  gateway:
    build:
      context: .
      dockerfile: src/gateway/Dockerfile
    container_name: gateway
    environment:
      # MinIO per DuckDB (httpfs)
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin

      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      AWS_EC2_METADATA_DISABLED: "true"
      AWS_S3_FORCE_PATH_STYLE: "true"
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ENDPOINT_URL: http://minio:9000
    ports:
      - "8237:8000"
    depends_on:
      - broker_kafka
      - minio
      - spark-master


# Machine learning
  build_features:
    build:
      context: .
      dockerfile: src/machine_learning/build_features/Dockerfile
    container_name: build_features
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  run_clustering:
    build:
      context: .
      dockerfile: src/machine_learning/run_clustering/Dockerfile
    container_name: run_clustering
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  generate_recommendations:
    build:
      context: .
      dockerfile: src/machine_learning/generate_recommendations/Dockerfile
    container_name: generate_recommendations
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1