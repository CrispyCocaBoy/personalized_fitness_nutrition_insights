services:

  # Frontend
  streamlit_app:
    build:
      context: .
      dockerfile: ./src/frontend/Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./src/frontend:/app
      - ./utility:/app/utility
    depends_on:
      - cockroachdb

# Ingestion phase
## MQTT protocol
  emqx1:
    image: emqx/emqx-enterprise:5.10.0
    container_name: emqx1

    environment:
      # --- SINGLE NODE ---
      EMQX_NODE__NAME: emqx@node1.emqx.local
      EMQX_CLUSTER__DISCOVERY_STRATEGY: manual
      # oppure elimina proprio tutte le variabili di cluster

      # --- DASHBOARD ---
      EMQX_DASHBOARD__DEFAULT_USERNAME: admin
      EMQX_DASHBOARD__DEFAULT_PASSWORD: public

      # --- KAFKA CONNECTOR (nome: kafka_producer) ---
      # endpoint del broker Kafka dal punto di vista DI EMQX (stesso network Docker)
      EMQX_CONNECTORS__KAFKA__kafka_producer__SERVERS: broker_kafka:9092
      # opzionali: auth/sasl se servono
      # EMQX_CONNECTORS__KAFKA__kafka_producer__SASL__MECHANISM: plain
      # EMQX_CONNECTORS__KAFKA__kafka_producer__SASL__USERNAME: xxx
      # EMQX_CONNECTORS__KAFKA__kafka_producer__SASL__PASSWORD: xxx

      # --- THROUGHPUT & BACKPRESSURE (alzano la capacit√† del replay queue) ---
      # NOTA: a seconda della versione, alcuni key possono essere sotto BRIDGES anzich√© CONNECTORS.
      # Li dichiaro in entrambi i posti per compatibilit√†.
      EMQX_CONNECTORS__KAFKA__kafka_producer__WORKER_POOL_SIZE: 8
      EMQX_CONNECTORS__KAFKA__kafka_producer__MAX_INFLIGHT: 2000
      EMQX_CONNECTORS__KAFKA__kafka_producer__BATCH_SIZE: 500
      EMQX_CONNECTORS__KAFKA__kafka_producer__LINGER_MS: 50
      EMQX_CONNECTORS__KAFKA__kafka_producer__RETRIES: 10
      EMQX_CONNECTORS__KAFKA__kafka_producer__REPLAYQ__CAPACITY: 100000
      EMQX_CONNECTORS__KAFKA__kafka_producer__REPLAYQ__SEGMENT_BYTES: 10485760

      # (duplicati su BRIDGES per alcune build)
      EMQX_BRIDGES__KAFKA__kafka_producer__SERVERS: broker_kafka:9092
      EMQX_BRIDGES__KAFKA__kafka_producer__WORKER_POOL_SIZE: 8
      EMQX_BRIDGES__KAFKA__kafka_producer__MAX_INFLIGHT: 2000
      EMQX_BRIDGES__KAFKA__kafka_producer__BATCH_SIZE: 500
      EMQX_BRIDGES__KAFKA__kafka_producer__LINGER_MS: 50
      EMQX_BRIDGES__KAFKA__kafka_producer__RETRIES: 10
      EMQX_BRIDGES__KAFKA__kafka_producer__REPLAYQ__CAPACITY: 100000
      EMQX_BRIDGES__KAFKA__kafka_producer__REPLAYQ__SEGMENT_BYTES: 10485760

      # --- (OPZIONALE) crea direttamente un bridge statico verso un topic ---
      # Se usi Rules Engine per decidere i topic dinamicamente, lascia vuoto;
      # se vuoi un bridge "semplice" di test:
      # EMQX_BRIDGES__KAFKA__bridge_test__CONNECTOR: kafka_producer
      # EMQX_BRIDGES__KAFKA__bridge_test__TOPIC: wearables.ppg.raw

    healthcheck:
      # controlla la dashboard; pi√π tollerante (evita "unhealthy" fasulli)
      test: [ "CMD", "sh", "-lc", "curl -fsS http://127.0.0.1:18083/status | grep -q 'running'" ]
      interval: 30s
      timeout: 5s
      retries: 20
      start_period: 60s

    ports:
      - 1883:1883     # MQTT
      - 8083:8083     # WebSocket
      - 8084:8084     # WSS
      - 8883:8883     # MQTT over TLS
      - 18083:18083   # Dashboard
    volumes:
      - ./volumes/emqx_data/config/cluster.hocon:/opt/emqx/data/configs/

## Kafka
  broker_kafka:
    image: apache/kafka:latest
    container_name: broker_kafka
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://broker_kafka:9092,CONTROLLER://broker_kafka:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker_kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker_kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - ./volumes/kafka_setting/broker:/var/lib/kafka

  init_kafka:
    build:
      context: .
      dockerfile: ./src/kafka_topics/Dockerfile
    depends_on:
      - broker_kafka
    container_name: init_kafka


  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8082:8080
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
    volumes:
      - ./volumes/kafka_setting/kui/config.yml:/etc/kafkaui/dynamic_config.yaml
    depends_on:
      - broker_kafka

# Databases
  # SQL-type
  cockroachdb:
    image: cockroachdb/cockroach:v25.2.0
    container_name: cockroachdb
    command: start-single-node --insecure --http-addr=0.0.0.0:8080
    ports:
      - "26257:26257" # Porta SQL
      - "8087:8080"   # Admin UI
    volumes:
      - ./volumes/cockroach_data:/cockroach/cockroach-data

  # Datalakehouse
  minio:
    image: minio/minio
    ports:
      - "9000:9000"      # API
      - "9001:9001"      # Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./volumes/minio_setting:/data

  # Cache
  ## Redis community
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"         # Porta standard Redis
    volumes:
      - ./volumes/redis_data:/data
    restart: unless-stopped

  ## UI for Redis
  redisinsight:
    image: redis/redisinsight
    container_name: redisinsight
    ports:
      - "5540:5540"
    volumes:
      - ./volumes/redis_data:/data

  # Change Data Capture
  ## Kafka connector
  change_data_capture:
    build:
      context: ./src/change_data_capture
      dockerfile: Dockerfile
    container_name: change_data_capture
    depends_on:
      - broker_kafka
      - redis
    environment:
      KAFKA_BOOTSTRAP: broker_kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: 6379

  # Data Generator
  fake_user:
    build:
      context: .
      dockerfile: ./src/data_generator/producer_user_generator/Dockerfile
    container_name: fake_user
    volumes:
      - ./database_utility/fake_user_and_password:/app/output/
    depends_on:
      - cockroachdb

  people_simulator:
    build:
      context: .
      dockerfile: src/data_generator/sensors/people_simulator/Dockerfile
    container_name: people_simulator
    depends_on:
      - emqx1
      - bronze_layer
      - redis

  sensor_coherent_simulator:
    build:
      context: .
      dockerfile: src/data_generator/sensors/sensor_coherent_simulator/Dockerfile
    container_name: sensor_coherent_simulator
    depends_on:
      - emqx1
      - bronze_layer

# ETL process
  ## Spark Master
  spark-master:
    image: bitnamilegacy/spark:latest
    container_name: spark-master
    command: /opt/bitnami/scripts/spark/run.sh master
    environment:
      - SPARK_MODE=master
    ports:
      - "8085:8080"
      - "7077:7077"

  ## Spark Worker 1
  spark-worker-1:
    image: bitnamilegacy/spark:latest
    container_name: spark-worker-1
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master

  ## Spark Worker 2
  spark-worker-2:
    image: bitnamilegacy/spark:latest
    container_name: spark-worker-2
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master

  ## Bronze layer job
  bronze_layer:
    build:
      context: .
      dockerfile: src/delta/bronze/sensors_consumers/Dockerfile
    container_name: bronze_layer
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  ## Silver layer job
  silver_layer:
    build:
      context: .
      dockerfile: src/delta/silver/sensor_silver/Dockerfile
    container_name: silver_layer
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  ## gold layer job
  gold_layer:
    build:
      context: .
      dockerfile: src/delta/gold/sensor_gold/Dockerfile
    container_name: gold_layer
    depends_on:
      - broker_kafka
      - silver_layer
      - spark-master
      - emqx1

  gold_meal:
    build:
      context: .
      dockerfile: src/delta/gold/meal_gold/Dockerfile
    container_name: gold_meal
    depends_on:
      - broker_kafka
      - silver_layer
      - spark-master
      - emqx1

  gold_activity:
    build:
      context: .
      dockerfile: src/delta/gold/activity_gold/Dockerfile
    container_name: gold_activity
    depends_on:
      - broker_kafka
      - silver_layer
      - spark-master
      - emqx1


  ## Latency monitor
#  latency-monitor:
#   build:
#     context: .
#      dockerfile: src/latency_monitor/Dockerfile
#    container_name: latency-monitor
#    depends_on:
#      - spark-master
#      - minio
#      - bronze_layer

# Gateway
  gateway:
    build:
      context: .
      dockerfile: src/gateway/Dockerfile
    container_name: gateway
    environment:
      # MinIO per DuckDB (httpfs)
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin

      # üëá Forza il kernel Delta a NON usare IMDS e a puntare MinIO
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      AWS_EC2_METADATA_DISABLED: "true"
      AWS_S3_FORCE_PATH_STYLE: "true"
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ENDPOINT_URL: http://minio:9000
    ports:
      - "8237:8000"
    depends_on:
      - broker_kafka
      - minio
      - spark-master


