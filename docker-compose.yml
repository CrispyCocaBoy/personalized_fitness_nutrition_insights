# ====================================================================
# Docker Compose - VERSIONE FINALE, COMPLETA E STABILE
# ====================================================================
services:

  # --------------------------------------------------------------------
  # 1. Servizi di Infrastruttura
  # --------------------------------------------------------------------
  cockroachdb:
    image: cockroachdb/cockroach:v25.2.0
    container_name: cockroachdb
    command: start-single-node --insecure --http-addr=0.0.0.0:8080
    ports: ["26257:26257", "8087:8080"]
    volumes:
      - ./volumes/cockroach_data:/cockroach/cockroach-data
    networks:
      - app-net

  minio:
    image: minio/minio
    ports: ["9000:9000", "9001:9001"]
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./volumes/minio_setting:/data
    networks:
      - app-net

  redis:
    image: redis:latest
    container_name: redis
    ports: ["6379:6379"]
    volumes:
      - ./volumes/redis_data:/data
    restart: unless-stopped
    networks:
      - app-net

  # --------------------------------------------------------------------
  # 2. Cluster di Elaborazione Spark
  # --------------------------------------------------------------------
  spark-master:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports: ["8085:8080", "7077:7077"]
    networks:
      - app-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  spark-worker-1:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - app-net

  spark-worker-2:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - app-net

  # --------------------------------------------------------------------
  # 3. Job di Training e Inferenza
  # --------------------------------------------------------------------
  build_features_job_workout:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: build_features_job_workout
    depends_on:
      - spark-master
      - minio
    volumes:
      - ./data/raw:/app/data/raw
    command: >
      /opt/bitnami/python/bin/python3 /app/src/feature_engineering/build_features_workout.py
    networks:
      - app-net

  clustering_job_workout:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: clustering_job_workout
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      minio:
        condition: service_started
    command: >
      spark-submit
      --master spark://spark-master:7077
      --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      /app/src/training/run_clustering_workout.py 
    networks:
      - app-net

  classification_job_workout:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: classification_job_workout
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      minio:
        condition: service_started
    command: >
      spark-submit
      --master spark://spark-master:7077
      --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      /app/src/training/train_classifiers_workout.py
    networks:
      - app-net

  recommendation_job_workout:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile # Usiamo lo stesso Dockerfile del training
    container_name: recommendation_job_workout
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      minio:
        condition: service_started
      cockroachdb: # <-- NUOVA DIPENDENZA
        condition: service_started
    command: >
      spark-submit
      --master spark://spark-master:7077
      --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.5.0
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      /app/src/inference/generate_recommendations_workout.py # Percorso allo script workout
    networks:
      - app-net

  build_features_job_nutrition:   
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: build_features_job_nutrition
    depends_on:
      - minio
    volumes:
      - ./data/raw:/app/data/raw
    command: >
      /opt/bitnami/python/bin/python3 /app/src/feature_engineering/build_features_nutrition.py
    networks:
      - app-net
  clustering_job_nutrition:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: clustering_job_nutrition
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      minio:
        condition: service_started
    command: >
      spark-submit
      --master spark://spark-master:7077
      --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      /app/src/training/run_clustering_nutrition.py
    networks:
      - app-net

  classification_job_nutrition:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: classification_job_nutrition
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      minio:
        condition: service_started
    command: >
      spark-submit
      --master spark://spark-master:7077
      --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      /app/src/training/train_classifiers_nutrition.py
    networks:
      - app-net
      

  recommendation_job_nutrition:
    build:
      context: .
      dockerfile: ./src/training/Dockerfile
    container_name: recommendation_job_nutrition
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      minio:
        condition: service_started
      cockroachdb: # <-- NUOVA DIPENDENZA
        condition: service_started
    command: >
      spark-submit
      --master spark://spark-master:7077
      --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.5.0
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      /app/src/inference/generate_recommendations_nutrition.py # Percorso allo script nutrition
    networks:
      - app-net
  # --------------------------------------------------------------------
  # 4. Frontend
  # --------------------------------------------------------------------
  streamlit_app:
    build:
      context: .
      dockerfile: ./src/frontend/Dockerfile
    ports: ["8501:8501"]
    volumes:
      - ./src/frontend:/app
      - ./utility:/app/utility
    depends_on: 
      - cockroachdb
    networks:
      - app-net

# ====================================================================
# Definizione della Rete Condivisa
# ====================================================================
networks:
  app-net:
    driver: bridge