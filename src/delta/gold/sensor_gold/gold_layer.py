# jobs/gold_metrics_with_daily_optimized.py
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, expr, current_timestamp, when, avg, stddev,
    collect_list, window, lit, to_date, coalesce,
    sum as Fsum, avg as Favg, min as Fmin, max as Fmax, count as Fcount
)
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, LongType, FloatType, IntegerType
)
from pyspark.sql.functions import udf
from delta.tables import DeltaTable

# ========================
# Config & paths
# ========================
S3_ENDPOINT     = os.getenv("S3_ENDPOINT", "http://minio:9000")
S3_ACCESS_KEY   = os.getenv("S3_ACCESS_KEY", "minioadmin")
S3_SECRET_KEY   = os.getenv("S3_SECRET_KEY", "minioadmin")
S3_BUCKET_GOLD  = os.getenv("S3_BUCKET_GOLD", "gold")
USERS_BASE      = f"s3a://{S3_BUCKET_GOLD}"
CKP_STREAM      = f"s3a://{S3_BUCKET_GOLD}/checkpoints/final_metrics_per_user/"
KAFKA_SERVERS   = os.getenv("KAFKA_BOOTSTRAP", "broker_kafka:9092")
SILVER_TOPIC    = os.getenv("SILVER_TOPIC", "silver_layer")
START_OFFSETS   = os.getenv("STARTING_OFFSETS", "latest")
FAIL_ON_LOSS    = os.getenv("FAIL_ON_DATA_LOSS", "false")
TRIGGER         = os.getenv("TRIGGER", "30 seconds")
MAX_OFFSETS     = os.getenv("MAX_OFFSETS_PER_TRIGGER", "")  # es. "50000"
USER_WEIGHT_KG  = float(os.getenv("USER_WEIGHT_KG", "75"))   # peso in kg usato nella formula kcal

FACT_PATH       = f"{USERS_BASE}/metrics_fact"       # partizioni: user_id, event_date
DAILY_PATH      = f"{USERS_BASE}/metrics_daily"      # chiave: (user_id, event_date)

# ========================
# Spark & Delta
# ========================
spark = (
    SparkSession.builder
    .appName("GoldLayer (Fact + Daily) ‚Äî Optimized")
    .config("spark.sql.session.timeZone", "UTC")  # importante per le finestre
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    # Tuning parallelismo (cluster: 2 worker x 2 core = 4 core totali)
    .config("spark.sql.shuffle.partitions", "8")
    .config("spark.default.parallelism", "8")
    # Delta write compattanti (OSS compatibile)
    .config("spark.databricks.delta.optimizeWrite.enabled", "true")
    .config("spark.databricks.delta.autoCompact.enabled", "true")
    # Limitazione dimensione file per ridurre small files
    .config("spark.sql.files.maxRecordsPerFile", "500000")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# S3A / MinIO
hadoopConf = spark._jsc.hadoopConfiguration()
hadoopConf.set("fs.s3a.access.key", S3_ACCESS_KEY)
hadoopConf.set("fs.s3a.secret.key", S3_SECRET_KEY)
hadoopConf.set("fs.s3a.endpoint", S3_ENDPOINT)
hadoopConf.set("fs.s3a.path.style.access", "true")
hadoopConf.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
# Tuning utili su MinIO
hadoopConf.set("fs.s3a.fast.upload", "true")
hadoopConf.set("fs.s3a.connection.maximum", "200")
hadoopConf.set("fs.s3a.threads.max", "200")
hadoopConf.set("fs.s3a.multipart.size", "134217728")  # 128MB
hadoopConf.set("fs.s3a.fast.upload.buffer", "disk")

print("‚úÖ Spark ready & MinIO configured (UTC)")

# ========================
# Schema Silver
# ========================
silver_schema = StructType([
    StructField("sensor_id", StringType()),
    StructField("timestamp", LongType()),            # epoch ms
    StructField("metric", StringType()),
    StructField("value", DoubleType()),
    StructField("topic", StringType()),
    StructField("ingest_ts", StringType()),
    StructField("latency_sec", DoubleType()),
    StructField("measurement_type", StringType()),   # ppg / accelerometer / ceda
    StructField("user_id", StringType())
])

df_silver_base = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_SERVERS)
    .option("subscribe", SILVER_TOPIC)
    .option("startingOffsets", START_OFFSETS)
    .option("failOnDataLoss", FAIL_ON_LOSS)
)
if MAX_OFFSETS.strip():
    df_silver_base = df_silver_base.option("maxOffsetsPerTrigger", MAX_OFFSETS.strip())

df_silver = (
    df_silver_base.load()
    .selectExpr("CAST(value AS STRING) AS json")
    .select(from_json(col("json"), silver_schema).alias("data"))
    .select("data.*")
    .withColumn("datetime", expr("from_unixtime(timestamp/1000)").cast("timestamp"))
)

# ========================
# UDF PPG: HR (bpm) + HRV (ms)
# ========================
def _hr_hrv_from_ppg(values, timestamps_ms):
    try:
        import numpy as np
        if not values or not timestamps_ms or len(values) < 20:
            return (None, None)
        v = np.asarray(values, dtype=float)
        t = np.asarray(timestamps_ms, dtype=float)

        # robust detrend/scale
        v = v - np.median(v)
        mad = np.median(np.abs(v)) + 1e-8
        v = v / (mad * 6.0)

        dt = np.diff(t); dt = dt[dt > 0]
        if dt.size == 0:
            return (None, None)
        fs = 1000.0 / float(np.median(dt))
        thr = float(np.mean(v) + 0.4*np.std(v))
        min_rr_n = max(1, int(0.30 * fs))

        peaks, last = [], -10**9
        for i in range(1, len(v)-1):
            if v[i] > thr and v[i] > v[i-1] and v[i] >= v[i+1]:
                if i - last >= min_rr_n:
                    peaks.append(i); last = i
        if len(peaks) < 2:
            return (None, None)

        rr = []
        for k in range(1, len(peaks)):
            dtms = t[peaks[k]] - t[peaks[k-1]]
            if 300.0 < dtms < 2000.0:
                rr.append(dtms)
        if len(rr) < 2:
            return (None, None)

        import numpy as _np
        rr = _np.asarray(rr, dtype=float)
        hr_raw = 60000.0 / float(_np.mean(rr))
        hrv_ms = float(_np.std(rr, ddof=1))  # SDNN (ms)

        # clamp HR plausibile (correttivo +20 applicato dopo)
        hr_raw = float(_np.clip(hr_raw, 35.0, 210.0))
        return (hr_raw, hrv_ms)
    except Exception:
        return (None, None)

from pyspark.sql.types import StructType as _Struct, StructField as _Field
hr_struct = _Struct([
    _Field("hr_bpm_raw", FloatType()),
    _Field("hrv_ms", FloatType())
])
hr_hrv_udf = udf(_hr_hrv_from_ppg, hr_struct)

# ========================
# ForeachBatch: FACT + DAILY
# ========================
def process_batch(batch_df, batch_id):
    if batch_df.rdd.isEmpty():
        print(f"‚ö†Ô∏è Batch {batch_id} vuoto")
        return

    # --- PPG: HR/HRV + SPO2 proxy + Stress ---
    ppg_agg = (
        batch_df
        .filter(col("measurement_type") == "ppg")
        .withWatermark("datetime", "2 minutes")
        .groupBy(col("user_id"), window(col("datetime"), "1 minute"))
        .agg(
            collect_list("value").alias("ppg_values"),
            collect_list("timestamp").alias("ppg_ts"),
            avg(when(col("metric") == "ppg_ir",  col("value"))).alias("ir_avg"),
            avg(when(col("metric") == "ppg_red", col("value"))).alias("red_avg"),
            stddev("value").alias("ppg_std")
        )
        .withColumn("window_start", col("window.start"))
        .withColumn("window_end", col("window.end"))
        .drop("window")
    )

    # HR + HRV
    ppg_with_hr = ppg_agg.withColumn("hr_pack", hr_hrv_udf(col("ppg_values"), col("ppg_ts")))
    ppg_with_hr = (
        ppg_with_hr
        .withColumn("hr_bpm_raw", col("hr_pack.hr_bpm_raw"))
        .withColumn("hrv_ms", col("hr_pack.hrv_ms"))
        .drop("hr_pack")
        .withColumn("hr_bpm", when(col("hr_bpm_raw").isNotNull(), col("hr_bpm_raw") + lit(20.0)).otherwise(None))  # correttivo +20
    )

    # SPO2 proxy
    spo2_dual = expr("least(100.0, greatest(85.0, 110.0 - 25.0 * (red_avg/ir_avg)))")
    spo2_fallback = expr("least(100.0, greatest(90.0, 95.0 + 5.0*(1 - (ppg_std / greatest(1e-6, abs(ppg_std)*10)))))")

    # Stress: HRV-first, fallback PPG std
    stress_by_hrv = (
        when(col("hrv_ms").isNotNull() & (col("hrv_ms") <= 20), lit("high"))      # molto alta variabilit√† = stress alto
        .when(col("hrv_ms").isNotNull() & (col("hrv_ms") <= 35), lit("medium"))   # media variabilit√† = stress medio  
        .when(col("hrv_ms").isNotNull() & (col("hrv_ms") <= 50), lit("medium"))   # normale variabilit√† = stress medio
        .when(col("hrv_ms").isNotNull(), lit("low"))                              # alta variabilit√† = stress basso
        .otherwise(lit(None))
    )
    stress_fallback_ppg = (
        when(col("ppg_std") < 0.01, lit("low"))      # soglia pi√π bassa per low stress
        .when(col("ppg_std") < 0.035, lit("medium")) # soglia pi√π bassa per medium stress
        .otherwise(lit("high"))
    )

    ppg_win = (
        ppg_with_hr
        .withColumn("spo2",
            when(col("ir_avg").isNotNull() & col("red_avg").isNotNull(), spo2_dual)
            .otherwise(spo2_fallback)
        )
        .withColumn(
            "stress_level",
            when(stress_by_hrv.isNotNull(), stress_by_hrv).otherwise(stress_fallback_ppg)
        )
        .select("user_id","window_start","window_end","hr_bpm","hrv_ms","spo2","stress_level","ppg_std")
    )

    # --- Accelerometro: steps proxy ---
    acc_win = (
        batch_df
        .filter(col("measurement_type") == "accelerometer")
        .withWatermark("datetime", "2 minutes")
        .groupBy(col("user_id"), window(col("datetime"), "1 minute"))
        .agg(
            stddev("value").alias("acc_std"),
            Fcount("*").alias("acc_samples")
        )
        .withColumn("window_start", col("window.start"))
        .withColumn("window_end", col("window.end"))
        .drop("window")
        .withColumn("step_count", expr("CAST(greatest(0.0, (acc_std - 0.08)) * 180 AS INT)"))
        .select("user_id","window_start","window_end","step_count")
    )

    # --- Join per finestra (ppg base + acc) ---
    base = (
        ppg_win.alias("p")
        .join(
            acc_win.alias("a"),
            (col("p.user_id")==col("a.user_id")) &
            (col("p.window_start")==col("a.window_start")) &
            (col("p.window_end")==col("a.window_end")),
            "left"
        )
        .select(
            col("p.user_id").alias("user_id"),
            col("p.window_start").alias("window_start"),
            col("p.window_end").alias("window_end"),
            col("p.hr_bpm").alias("hr_bpm"),
            col("p.hrv_ms").alias("hrv_ms"),
            col("p.spo2").alias("spo2"),
            col("a.step_count").alias("step_count"),
            col("p.stress_level").alias("stress_level")
        )
    )

    # --- Calorie (kcal) con formula richiesta ---
    # KCAL_T = T * peso * 0.85 * (25 + HRV/6 + (BPM-60)/12) / 24 + passi * 0.03
    # T in ore per finestra di 1 minuto = ~1/60 h (calcolato da window_end/start per robustezza)
    gold = (
        base
        .withColumn("event_date", to_date(col("window_start")))
        .withColumn("T_hours", (expr("unix_timestamp(window_end) - unix_timestamp(window_start)"))/lit(3600.0))
        .withColumn("weight", lit(USER_WEIGHT_KG))
        .withColumn("hrv_used", coalesce(col("hrv_ms"), lit(50.0)))   # fallback ragionevole
        .withColumn("bpm_used", coalesce(col("hr_bpm"), lit(70.0)))
        .withColumn("steps_used", coalesce(col("step_count"), lit(0)))
        .withColumn(
            "calories",
            expr("""
                T_hours * weight * 0.85 * (25 + hrv_used/6.0 + (bpm_used - 60)/12.0) / 24.0
            """) + col("steps_used") * lit(0.03)
        )
        .withColumn("gold_ingest_ts", current_timestamp())
        .select("user_id","window_start","window_end","event_date",
                "hr_bpm","hrv_ms","spo2","step_count","stress_level","calories","gold_ingest_ts")
    )

    if gold.rdd.isEmpty():
        print(f"‚ö†Ô∏è Batch {batch_id}: nessuna finestra chiusa")
        return

    # ===========
    # CAST (stabilizza schema)
    # ===========
    gold_cast = (
        gold
        .withColumn("hr_bpm",     col("hr_bpm").cast(DoubleType()))
        .withColumn("hrv_ms",     col("hrv_ms").cast(DoubleType()))
        .withColumn("spo2",       col("spo2").cast(DoubleType()))
        .withColumn("step_count", col("step_count").cast(IntegerType()))
        .withColumn("calories",   col("calories").cast(DoubleType()))
    )

    # ===========
    # FACT (append coerente)
    # ===========
    (gold_cast.write
         .format("delta")
         .mode("append")
         .option("mergeSchema", "true")
         .partitionBy("user_id", "event_date")
         .save(FACT_PATH))

    # ===========
    # DAILY (build dal gold_cast)
    # ===========
    daily = (
        gold_cast.groupBy("user_id", "event_date")
            .agg(
                Favg("hr_bpm").cast(DoubleType()).alias("hr_bpm_avg"),
                Fmin("hr_bpm").cast(DoubleType()).alias("hr_bpm_min"),
                Fmax("hr_bpm").cast(DoubleType()).alias("hr_bpm_max"),
                Favg("spo2").cast(DoubleType()).alias("spo2_avg"),

                Fsum(coalesce(col("step_count"), lit(0))).cast(IntegerType()).alias("steps_total"),
                Fsum(coalesce(col("calories"), lit(0.0))).cast(DoubleType()).alias("calories_total"),

                Fsum(when(col("stress_level")=="low",    1).otherwise(0)).cast(IntegerType()).alias("stress_low_cnt"),
                Fsum(when(col("stress_level")=="medium", 1).otherwise(0)).cast(IntegerType()).alias("stress_medium_cnt"),
                Fsum(when(col("stress_level")=="high",   1).otherwise(0)).cast(IntegerType()).alias("stress_high_cnt"),

                Fcount(lit(1)).cast(IntegerType()).alias("windows_count"),
                Fmax("window_end").alias("last_window_end")
            )
            .withColumn("dominant_stress",
                expr("""
                    case
                      when stress_high_cnt >= greatest(stress_medium_cnt, stress_low_cnt) then 'high'
                      when stress_medium_cnt >= greatest(stress_high_cnt, stress_low_cnt) then 'medium'
                      else 'low'
                    end
                """)
            )
            .withColumn("gold_ingest_ts", current_timestamp())
    )

    # ===========
    # MERGE incrementale sulla DAILY
    # ===========
    if DeltaTable.isDeltaTable(spark, DAILY_PATH):
        tgt = DeltaTable.forPath(spark, DAILY_PATH)
        (tgt.alias("t")
            .merge(
                daily.alias("s"),
                "t.user_id = s.user_id AND t.event_date = s.event_date"
            )
            # Update incrementale: somme per cumulativi, medie ponderate per avg
            .whenMatchedUpdate(set = {
                # medie ponderate (evita nullif per compat): denom = t.windows_count + s.windows_count
                "hr_bpm_avg": expr("""
                    CASE WHEN (t.windows_count + s.windows_count) = 0
                         THEN COALESCE(s.hr_bpm_avg, t.hr_bpm_avg)
                         ELSE (COALESCE(t.hr_bpm_avg,0) * t.windows_count + COALESCE(s.hr_bpm_avg,0) * s.windows_count)
                              / (t.windows_count + s.windows_count)
                    END
                """),
                "hr_bpm_min": expr("least(COALESCE(t.hr_bpm_min, 999), COALESCE(s.hr_bpm_min, 999))"),
                "hr_bpm_max": expr("greatest(COALESCE(t.hr_bpm_max, 0), COALESCE(s.hr_bpm_max, 0))"),
                "spo2_avg": expr("""
                    CASE WHEN (t.windows_count + s.windows_count) = 0
                         THEN COALESCE(s.spo2_avg, t.spo2_avg)
                         ELSE (COALESCE(t.spo2_avg,0) * t.windows_count + COALESCE(s.spo2_avg,0) * s.windows_count)
                              / (t.windows_count + s.windows_count)
                    END
                """),

                # FIX: passi devono essere INCREMENTATI, non sostituiti
                "steps_total":     expr("COALESCE(t.steps_total, 0) + COALESCE(s.steps_total, 0)"),
                "calories_total":  expr("COALESCE(t.calories_total, 0) + COALESCE(s.calories_total, 0)"),

                "stress_low_cnt":    expr("COALESCE(t.stress_low_cnt, 0) + COALESCE(s.stress_low_cnt, 0)"),
                "stress_medium_cnt": expr("COALESCE(t.stress_medium_cnt, 0) + COALESCE(s.stress_medium_cnt, 0)"),
                "stress_high_cnt":   expr("COALESCE(t.stress_high_cnt, 0) + COALESCE(s.stress_high_cnt, 0)"),

                "windows_count":   expr("COALESCE(t.windows_count, 0) + COALESCE(s.windows_count, 0)"),
                "last_window_end": expr("greatest(COALESCE(t.last_window_end, s.last_window_end), COALESCE(s.last_window_end, t.last_window_end))"),

                # Ricalcolo dominant_stress dopo l'incremento
                "dominant_stress": expr("""
                    case
                      when (COALESCE(t.stress_high_cnt,0) + COALESCE(s.stress_high_cnt,0)) >= 
                           greatest(COALESCE(t.stress_medium_cnt,0) + COALESCE(s.stress_medium_cnt,0),
                                   COALESCE(t.stress_low_cnt,0) + COALESCE(s.stress_low_cnt,0)) then 'high'
                      when (COALESCE(t.stress_medium_cnt,0) + COALESCE(s.stress_medium_cnt,0)) >= 
                           greatest(COALESCE(t.stress_high_cnt,0) + COALESCE(s.stress_high_cnt,0),
                                   COALESCE(t.stress_low_cnt,0) + COALESCE(s.stress_low_cnt,0)) then 'medium'
                      else 'low'
                    end
                """),
                "gold_ingest_ts": current_timestamp()
            })
            .whenNotMatchedInsertAll()
            .execute())
    else:
        (daily.write
              .format("delta")
              .mode("overwrite")
              .option("overwriteSchema", "true")
              .save(DAILY_PATH))

    print(f"‚úÖ Batch {batch_id}: wrote FACT ‚Üí {FACT_PATH} | merged DAILY ‚Üí {DAILY_PATH}")

# ========================
# Streaming query
# ========================
query_builder = (
    df_silver.writeStream
    .foreachBatch(process_batch)
    .outputMode("append")  # richiesto dall‚ÄôAPI
    .trigger(processingTime=TRIGGER)
    .option("checkpointLocation", CKP_STREAM)
)

query = query_builder.start()
print(f"üöÄ Gold (fact + daily) avviato ‚Äî FACT: {FACT_PATH} | DAILY: {DAILY_PATH}")

try:
    query.awaitTermination()
except KeyboardInterrupt:
    print("‚èπ Stop richiesto")
    query.stop()
    spark.stop()