services:

  # Frontend
  streamlit_app:
    build:
      context: .
      dockerfile: ./src/frontend/Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./src/frontend:/app
      - ./utility:/app/utility
    depends_on:
      - cockroachdb


## Kafka
  broker_kafka:
    image: apache/kafka:latest
    container_name: broker_kafka
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://broker_kafka:9092,CONTROLLER://broker_kafka:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker_kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker_kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
    volumes:
      - ./volumes/kafka_setting/broker:/var/lib/kafka

  init_kafka:
    build:
      context: .
      dockerfile: ./src/kafka_topics/Dockerfile
    depends_on:
      - broker_kafka
    container_name: init_kafka

# Databases

  # SQL-type
  cockroachdb:
    image: cockroachdb/cockroach:v25.2.0
    container_name: cockroachdb
    command: start-single-node --insecure --http-addr=0.0.0.0:8080
    ports:
      - "26257:26257" # Porta SQL
      - "8087:8080"   # Admin UI
    volumes:
      - ./volumes/cockroach_data:/cockroach/cockroach-data

  # Datalakehouse
  minio:
    image: minio/minio
    ports:
      - "9000:9000"      # API
      - "9001:9001"      # Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./volumes/minio_setting:/data

  # Cache
  ## Redis community
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"         # Porta standard Redis
    volumes:
      - ./volumes/redis_data:/data
    restart: unless-stopped



  # Change Data Capture
  ## Kafka connector
  change_data_capture_user_map:
    build:
      context: ./src/change_data_capture/CDC_user_map
      dockerfile: Dockerfile
    container_name: change_data_capture_user_map
    depends_on:
      - broker_kafka
      - redis
    environment:
      KAFKA_BOOTSTRAP: broker_kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: 6379

  change_data_capture_toggling_sensor:
    build:
      context: ./src/change_data_capture/CDC_toggling_sensor
      dockerfile: Dockerfile
    container_name: change_data_capture_toggling_sensor
    depends_on:
      - broker_kafka
      - redis
    environment:
      KAFKA_BOOTSTRAP: broker_kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: 6379



# ETL process
  ## Spark Master
  spark-master:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-master
    command: /opt/bitnami/scripts/spark/run.sh master
    environment:
      - SPARK_MODE=master
    ports:
      - "8085:8080"
      - "7077:7077"

  ## Spark Worker 1
  spark-worker-1:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-worker-1
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=3
    depends_on:
      - spark-master

  ## Spark Worker 2
  spark-worker-2:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-worker-2
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master

  spark-worker-3:
    image: bitnamilegacy/spark:3.5.5
    container_name: spark-worker-3
    command: /opt/bitnami/scripts/spark/run.sh worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master


# Gateway
  gateway:
    build:
      context: .
      dockerfile: src/gateway/Dockerfile
    container_name: gateway
    environment:
      # MinIO per DuckDB (httpfs)
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin

      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      AWS_EC2_METADATA_DISABLED: "true"
      AWS_S3_FORCE_PATH_STYLE: "true"
      AWS_ENDPOINT_URL_S3: http://minio:9000
      AWS_ENDPOINT_URL: http://minio:9000
    ports:
      - "8237:8000"
    depends_on:
      - broker_kafka
      - minio
      - spark-master


# Machine learning
  build_features:
    build:
      context: .
      dockerfile: src/machine_learning/build_features/Dockerfile
    container_name: build_features
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  run_clustering:
    build:
      context: .
      dockerfile: src/machine_learning/run_clustering/Dockerfile
    container_name: run_clustering
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  generate_recommendations:
    build:
      context: .
      dockerfile: src/machine_learning/generate_recommendations/Dockerfile
    container_name: generate_recommendations
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1



    # --- Build delle immagini job (una tantum, tag locali) ---
  build_features_image:
    build:
      context: .
      dockerfile: src/machine_learning/build_features/Dockerfile
    image: ml/build_features:latest

  run_clustering_image:
    build:
      context: .
      dockerfile: src/machine_learning/run_clustering/Dockerfile
    image: ml/run_clustering:latest

  train_classifiers_image:
    build:
      context: .
      dockerfile: src/machine_learning/train_classifiers/Dockerfile
    image: ml/train_classifiers:latest

  generate_recommendations_image:
    build:
      context: .
      dockerfile: src/machine_learning/generate_recommendations/Dockerfile
    image: ml/generate_recommendations:latest

  # --- Ofelia scheduler ---
  ofelia:
    image: mcuadros/ofelia:latest
    command: daemon --docker
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1
      - build_features_image
      - run_clustering_image
      - train_classifiers_image
      - generate_recommendations_image
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # opzionale: salva report esecuzioni
      - ./ofelia-reports:/reports
    labels:
      # === Global logging (facoltativo, salva report json) ===
      ofelia.global.save-folder: "/reports"
      ofelia.global.save-only-on-error: "false"

      # === JOBS ===
      # Nota: il formato cron di Ofelia include i SECONDI come 1° campo.

      # 1) build_features — ogni 15 minuti
      ofelia.job-run.build_features.schedule: "@every 15m"
      ofelia.job-run.build_features.image: "ml/build_features:latest"
      # Se il tuo Dockerfile ha già il CMD che lancia spark-submit, puoi omettere "command".
      # Per essere espliciti, specifico il comando (adatta se il tuo CMD è diverso):
      ofelia.job-run.build_features.command: >
        spark-submit --master spark://spark-master:7077 --deploy-mode client
        --name build_features /app/build_features.py
      ofelia.job-run.build_features.network: "bdt_net"
      ofelia.job-run.build_features.no-overlap: "true"

      # 2) run_clustering — ogni ora al minuto 05
      ofelia.job-run.run_clustering.schedule: "0 5 * * * *"
      ofelia.job-run.run_clustering.image: "ml/run_clustering:latest"
      ofelia.job-run.run_clustering.command: >
        spark-submit --master spark://spark-master:7077 --deploy-mode client
        --name run_clustering /app/run_clustering.py
      ofelia.job-run.run_clustering.network: "bdt_net"
      ofelia.job-run.run_clustering.no-overlap: "true"

      # 3) train_classifiers — tutti i giorni alle 02:30
      ofelia.job-run.train_classifiers.schedule: "0 30 2 * * *"
      ofelia.job-run.train_classifiers.image: "ml/train_classifiers:latest"
      ofelia.job-run.train_classifiers.command: >
        spark-submit --master spark://spark-master:7077 --deploy-mode client
        --name train_classifiers /app/train_classifiers.py
      ofelia.job-run.train_classifiers.network: "bdt_net"
      ofelia.job-run.train_classifiers.no-overlap: "true"

      # 4) generate_recommendations — ogni 10 minuti
      ofelia.job-run.generate_recommendations.schedule: "@every 10m"
      ofelia.job-run.generate_recommendations.image: "ml/generate_recommendations:latest"
      ofelia.job-run.generate_recommendations.command: >
        spark-submit --master spark://spark-master:7077 --deploy-mode client
        --name generate_recommendations /app/generate_recommendations.py
      ofelia.job-run.generate_recommendations.network: "bdt_net"
      ofelia.job-run.generate_recommendations.no-overlap: "true"



  # Machine learning
  build_features:
    build:
      context: .
      dockerfile: src/machine_learning/build_features/Dockerfile
    container_name: build_features
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  run_clustering:
    build:
      context: .
      dockerfile: src/machine_learning/run_clustering/Dockerfile
    container_name: run_clustering
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  generate_recommendations:
    build:
      context: .
      dockerfile: src/machine_learning/generate_recommendations/Dockerfile
    container_name: generate_recommendations
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1

  train_classifiers:
    build:
      context: .
      dockerfile: src/machine_learning/train_classifiers/Dockerfile
    container_name: train_classifiers
    depends_on:
      - broker_kafka
      - spark-master
      - emqx1
